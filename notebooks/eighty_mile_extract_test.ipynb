import datacube
from datetime import datetime
from datacube.storage import masking
import numpy as np
import pandas
import geopandas
from image_processing.segmentation import Segments, rasterise_vector
import fiona
import xarray as xr
import ogr
from os.path import basename, splitext
%matplotlib inline

vec_file = '/home/554/bh5260/Documents/eighty_mile_sites.shp'
acq_min = '2015-06'
acq_max = '2015-07'
product_type = 'nbar'
platform_list = ['ls8']

output_name = '{}_{}_{}_{}_{}.csv'.format('_'.join(platform_list), product_type, acq_min, acq_max, splitext(basename(vec_file))[0])
										  

src = fiona.open(vec_file, 'r')
lon_range = (src.bounds[0], src.bounds[2])
lat_range = (src.bounds[-1], src.bounds[1])
gdf = geopandas.read_file(vec_file)
gdf.plot()

valid_bit = 8

def pq_fuser(dest, src): 
    valid_val = (1 << valid_bit) 

    no_data_dest_mask = ~(dest & valid_val).astype(bool) 
    np.copyto(dest, src, where=no_data_dest_mask) 

    both_data_mask = (valid_val & dest & src).astype(bool) 
    np.copyto(dest, src & dest, where=both_data_mask)

def get_cloud_free_valid(product_type, platform, lon_range, lat_range, acq_min, acq_max):
# load data
    if product_type  in ['nbar', 'nbart']:
        # select measurements for NBAR/NBART, so that LS8 data can merge with others
        measurements_list = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']
		# make sure there are original products available in the database
    if pgn_data_orig:
	# get cloud free data for the polygon of interest
        pq = dc.load(product=pq_name, 
                 x=lon_range, y=lat_range, time=(acq_min, acq_max), 
                 group_by='solar_day', fuse_func=pq_fuser)
        cloud_free = masking.make_mask(pq, cloud_acca='no_cloud', cloud_fmask='no_cloud', contiguous=True).pixelquality
        pgn_cld_free = pgn_data_orig.where(cloud_free).dropna(dim='time')
        # remove nodata 
        pgn_cld_free_valid = masking.mask_valid_data(pgn_cld_free)
		 # make sure there are some original products left after PQ applied 
		 else:
        return 0, 0, 0

dc = datacube.Datacube(app='pgn_drill')

pgn_data_list = []
for platform in platform_list:
    pgn_data, data_affine, data_crs = get_cloud_free_valid(product_type, platform, lon_range, lat_range, acq_min, acq_max)
    # make sure there are products available returned from previous processing
    if pgn_data != 0:
        # Determine the pixel size in hectares. will be used in the statistics later on.
        ha = data_affine.a **2 / 10000.0
        print "Pixel area in hectares: {}".format(ha)
        dims = pgn_data.dims
        # Rasterise all the geometry contained within the vector file, and create a Segments object.
        ras = rasterise_vector(vec_file, shape=(dims['y'], dims['x']), crs=data_crs.wkt, transform=data_affine)
        seg = Segments(ras)
        print "Number of segments: {}".format(seg.n_segments)
        spectra = pgn_data.data_vars.keys()
        print "Available bands are: {}".format(str(spectra))
        # Calculate basic statistic info for each timestamp and each band
        zonal_stats = pandas.DataFrame()
        for ts in pgn_data.time:
            spectra_stats = pandas.DataFrame(columns=['Segment_IDs'])
            for sp in spectra:
                data = pgn_data[sp].sel(time=ts).values
                df = seg.basic_statistics(data, nan=True, scale_factor=ha, dataframe=True, label=sp)
                spectra_stats = pandas.merge(spectra_stats, df, on='Segment_IDs', how='outer')
            spectra_stats['timestamp'] = ts.values
            zonal_stats = zonal_stats.append(spectra_stats)
        zonal_stats.set_index('timestamp', inplace=True)
        # Add a platform column
        zonal_stats['platform'] = pandas.Series(platform, index=zonal_stats.index)
        # put the zonal_stats of each platform into a list for merging in next step
		 pgn_data_list.append(zonal_stats)

platform_merge = pandas.concat(pgn_data_list)
# move platform to the first colomn
cols = list(platform_merge)
cols.insert(0, cols.pop(cols.index('platform')))
platform_merge = platform_merge[cols]
# sort by time
platform_merge.sort_index(inplace=True)
platform_merge

platform_merge.to_csv(output_name)




